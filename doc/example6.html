<html>
<head>
<title>GPTL usage example 6: Automatic generation of MPI statistics</title>
<meta name="example" content="MPI profile">
<meta name="Keywords" content="pmpi","mpi","gptl","papi","call tree","profile","timing","performance analysis">
<meta name="Author" content="Jim Rosinski">
</head>
<body bgcolor="peachpuff">

<hr />
<a href="example5.html"><img border="0" src="btn_previous.gif"
				  width="100" height="20" alt="Example 4"
				  /></a> 
<a href="gptl_homepage.html"><img border="0" src="btn_next.gif"
			     width="100" height="20" alt="GPTL home page" /></a>

<br />

<h2>Example 6: Automatic generation of MPI statistics</h2>
This is a Fortran code which uses newly implemented option
<b>ENABLE_PMPI</b> (in macros.make) to automatically profile various MPI routines in
the call tree along with number of invocations and average number of bytes
transferred per call. The set of MPI routines which can be automatically
profiled in this way is currently limited to a subset of commonly called
routines (MPI_Send, MPI_Recv, MPI_Isend, MPI_Irecv, etc.). An easy
way to see the list of supported routines is to run ftests/pmpi and examine
output file timing.0 which it creates.
<p>
If you would like to add some routines, see files pmpi.c and
f_wrappers.c. I'd be glad to add any of interest to the library if you can
provide them. It's just a methodical and repetitive process of typing in the 
appropriate wrapping code.
<p>
In this example HAVE_IARGCGETARG=yes in macros.make. This declares
that libraries provided by the Fortran compiler or runtime contains entries
for functions <b>iargc()</b> and <b>getarg()</b>. If these functions are
available, <b>MPI_Init()</b> and <b>MPI_Finalize()</b> are automatically
instrumented with calls to <b>GPTLinitialize()</b>, and a wrapping timer for
the entire program. Likewise, <b>MPI_Finalize()</b> is automatically
instrumented to print timers when it is called. <b>GPTL</b> can therefore be
used to gather and print timing statistics with zero modifications to user
code. Potentially used in conjunction with the auto-instrumentation feature
of many compilers (e.g. -finstrument-functions in gcc), a large amount of
hopefully useful data can be gathered.
<p>
<b><em>pmpi.F90:</em></b>
<pre>
<div style="background-color:white;">
module myvars
  integer :: iam
end module myvars

program pmpi
  use myvars
  implicit none

#include &#60mpif.h&gt
#include "../gptl.inc"

  integer, parameter :: tag = 98
  integer, parameter :: count = 1024

  integer :: i, j, ret
  integer :: commsize
  integer :: val
  integer :: comm = MPI_COMM_WORLD
  integer :: sendbuf(0:count-1)
  integer :: recvbuf(0:count-1)
  integer :: sum
  integer :: status(MPI_STATUS_SIZE)
  integer :: request
  integer :: dest
  integer :: source
  integer :: displs(0:count-1)

  integer, allocatable :: atoabufsend(:)
  integer, allocatable :: atoabufrecv(:)
  integer, allocatable :: gsbuf(:,:)
  integer, allocatable :: recvcounts(:)

  ret = gptlsetoption (gptloverhead, 0)
  ret = gptlsetoption (gptlpercent, 0)
  ret = gptlsetoption (gptlabort_on_error, 1)
  ret = gptlsetoption (gptlsync_mpi, 1)

#if ( ! defined HAVE_IARGCGETARG )
  ret = gptlinitialize ()
  ret = gptlstart ("total")
#endif

  call mpi_init (ret)
  call mpi_comm_rank (comm, iam, ret)
  call mpi_comm_size (comm, commsize, ret)
  
  do i=0,count-1
    sendbuf(i) = iam
  end do

  dest = mod ((iam + 1), commsize)
  source = iam - 1
  if (source < 0) then
    source = commsize - 1
  end if

  if (mod (commsize, 2) == 0) then
    if (mod (iam, 2) == 0) then
      call mpi_send (sendbuf, count, MPI_INTEGER, dest, tag, comm, ret)
      call mpi_recv (recvbuf, count, MPI_INTEGER, source, tag, comm, status, ret)
    else
      call mpi_recv (recvbuf, count, MPI_INTEGER, source, tag, comm, status, ret)
      call mpi_send (sendbuf, count, MPI_INTEGER, dest, tag, comm, ret)
    end if
  end if
  call chkbuf ('mpi_send + mpi_recv', recvbuf(:), count, source)
  
  call mpi_sendrecv (sendbuf, count, MPI_INTEGER, dest, tag, &
                     recvbuf, count, MPI_INTEGER, source, tag, &
                     comm, status, ret)
  call chkbuf ('mpi_sendrecv', recvbuf(:), count, source)

  call mpi_irecv (recvbuf, count, MPI_INTEGER, source, tag, &
                  comm, request, ret)
  call mpi_isend (sendbuf, count, MPI_INTEGER, dest, tag, &
                  comm, request, ret)
  call mpi_wait (request, status, ret)
  call chkbuf ("mpi_wait", recvbuf(:), count, source)

  call mpi_irecv (recvbuf, count, MPI_INTEGER, source, tag, &
                  comm, request, ret)
  call mpi_isend (sendbuf, count, MPI_INTEGER, dest, tag, &
                  comm, request, ret)
  call mpi_waitall (1, request, status, ret)
  call chkbuf ("mpi_waitall", recvbuf(:), count, source)

  call mpi_barrier (comm, ret)

  call mpi_bcast (sendbuf, count, MPI_INTEGER, 0, comm, ret)
  call chkbuf ("MPI_Bcast", sendbuf(:), count, 0)

  do i=0,count-1
    sendbuf(i) = iam
  end do

  call mpi_allreduce (sendbuf, recvbuf, count, MPI_INTEGER, MPI_SUM, comm, ret)
  sum = 0.
  do i=0,commsize-1
    sum = sum + i
  end do
  call chkbuf ("mpi_allreduce", recvbuf(:), count, sum)

  allocate (gsbuf(0:count-1,0:commsize-1))
  allocate (recvcounts(0:commsize-1))
      
  call mpi_gather (sendbuf, count, MPI_INTEGER, &
                   gsbuf, count, MPI_INTEGER, 0, comm, ret)
  if (iam == 0) then
    do j=0,commsize-1
      do i=0,count-1
        if (gsbuf(i,j) /= j) then
          write(6,*) "iam=", iam, "MPI_Gather: bad gsbuf(", i,",",j,")=", &
                     gsbuf(i,j), " not= ",j
          call mpi_abort (MPI_COMM_WORLD, -1, ret)
        end if
      end do
    end do
  end if
!
! mpi_gatherv: make just like mpi_gather for simplicity
!
  gsbuf(:,:) = 0
  recvcounts(:) = count
  displs(0) = 0
  do i=1,commsize-1
    displs(i) = displs(i-1) + recvcounts(i-1)
  end do
  call mpi_gatherv (sendbuf, count, MPI_INTEGER, &
                    gsbuf, recvcounts, displs, &
                    MPI_INTEGER, 0, comm, ret)
  if (iam == 0) then
    do j=0,commsize-1
      do i=0,count-1
        if (gsbuf(i,j) /= j) then
          write(6,*) "iam=", iam, "MPI_Gather: bad gsbuf(", i,",",j,")=", &
                     gsbuf(i,j), " not= ",j
          call mpi_abort (MPI_COMM_WORLD, -1, ret)
        end if
      end do
    end do
  end if

  call mpi_scatter (gsbuf, count, MPI_INTEGER, recvbuf, count, MPI_INTEGER, 0, comm, ret)
  call chkbuf ("MPI_Scatter", recvbuf(:), count, iam)

  allocate (atoabufsend(0:commsize-1))
  allocate (atoabufrecv(0:commsize-1))
  do i=0,commsize-1
    atoabufsend(i) = i
  end do

  call mpi_alltoall (atoabufsend, 1, MPI_INTEGER, atoabufrecv, 1, MPI_INTEGER, comm, ret)
  do i=0,commsize-1
    if (atoabufrecv(i) /= iam) then
      write(6,*) "iam=", iam, "MPI_Alltoall: bad atoabufrecv(",i,")=",atoabufrecv(i)
      call mpi_abort (MPI_COMM_WORLD, -1, ret)
    end if
  end do

  call mpi_reduce (sendbuf, recvbuf, count, MPI_INTEGER, MPI_SUM, 0, comm, ret)
  if (iam == 0) then
    sum = 0.
    do i=0,commsize-1
      sum = sum + i
    end do
    call chkbuf ("mpi_reduce", recvbuf(:), count, sum)
  end if

  gsbuf(:,:) = 0
  call mpi_allgather (sendbuf, count, MPI_INTEGER, &
                      gsbuf, count, MPI_INTEGER, comm, ret)
  do j=0,commsize-1
    do i=0,count-1
      if (gsbuf(i,j) /= j) then
        write(6,*) "iam=", iam, "MPI_Allgather: bad gsbuf(",i,",",j,")=", &
                    gsbuf(i,j)
        call mpi_abort (MPI_COMM_WORLD, -1, ret)
      end if
    end do
  end do
!
! mpi_allgatherv: Make just like mpi_allgather for simplicity
!
  gsbuf(:,:) = 0
  recvcounts(:) = count
  displs(0) = 0
  call mpi_allgatherv (sendbuf, count, MPI_INTEGER, &
                       gsbuf, recvcounts, displs, &
                       MPI_INTEGER, comm, ret)
  do j=0,commsize-1
    do i=0,count-1
      if (gsbuf(i,j) /= j) then
        write(6,*) "iam=", iam, "MPI_Allgatherv: bad gsbuf(",i,",",j,")=", &
                    gsbuf(i,j)
        call mpi_abort (MPI_COMM_WORLD, -1, ret)
      end if
    end do
  end do

  call mpi_finalize (ret)

#if ( ! defined HAVE_IARGCGETARG )
  ret = gptlstop ("total")
  ret = gptlpr (iam)
#endif

  stop 0
end program pmpi

subroutine chkbuf (msg, recvbuf, count, source)
  use myvars
  implicit none

#include &#60mpif.h&gt

  character(len=*), intent(in) :: msg

  integer, intent(in) :: count
  integer, intent(in) :: recvbuf(0:count-1)
  integer, intent(in) :: source

  integer :: i
  integer :: ret
  do i=0,count-1
    if (recvbuf(i) /= source) then
      write(6,*) "iam=", iam, msg, "bad recvbuf(", i,")=",recvbuf(i), "/= ", source
      call mpi_abort (MPI_COMM_WORLD, -1, ret)
    end if
  end do
end subroutine chkbuf
</div>
</pre>
Now compile and run:
<pre>
<div>
% mpif90 -fopenmp -o pmpi pmpi.F90 -lgptl
% mpiexec -n 2 ./pmpi
</div>
</pre>

Here's the output contained in file timing.0:
<pre>
<div style="background-color:white;">
Stats for thread 0:
                            Called  Recurse Wallclock max       min       AVG_MPI_BYTES 
  MPI_Init_thru_Finalize           1    -       0.000     0.000     0.000       -       
    MPI_Send                       1    -       0.000     0.000     0.000     4.096e+03 
    sync_Recv                      1    -       0.000     0.000     0.000       -       
    MPI_Recv                       1    -       0.000     0.000     0.000     4.096e+03 
    MPI_Sendrecv                   1    -       0.000     0.000     0.000     8.192e+03 
    MPI_Irecv                      2    -       0.000     0.000     0.000     4.096e+03 
    MPI_Isend                      2    -       0.000     0.000     0.000     4.096e+03 
    MPI_Wait                       1    -       0.000     0.000     0.000       -       
    MPI_Waitall                    1    -       0.000     0.000     0.000       -       
    MPI_Barrier                    1    -       0.000     0.000     0.000       -       
    sync_Bcast                     1    -       0.000     0.000     0.000       -       
    MPI_Bcast                      1    -       0.000     0.000     0.000     4.096e+03 
    sync_Allreduce                 1    -       0.000     0.000     0.000       -       
    MPI_Allreduce                  1    -       0.000     0.000     0.000     8.192e+03 
    sync_Gather                    1    -       0.000     0.000     0.000       -       
    MPI_Gather                     1    -       0.000     0.000     0.000     1.229e+04 
    sync_Gatherv                   1    -       0.000     0.000     0.000       -       
    MPI_Gatherv                    1    -       0.000     0.000     0.000     2.048e+04 
    sync_Scatter                   1    -       0.000     0.000     0.000       -       
    MPI_Scatter                    1    -       0.000     0.000     0.000     8.192e+03 
    sync_Alltoall                  1    -       0.000     0.000     0.000       -       
    MPI_Alltoall                   1    -       0.000     0.000     0.000     1.200e+01 
    sync_Reduce                    1    -       0.000     0.000     0.000       -       
    MPI_Reduce                     1    -       0.000     0.000     0.000     4.096e+03 
    sync_Allgather                 1    -       0.000     0.000     0.000       -       
    MPI_Allgather                  1    -       0.000     0.000     0.000     1.229e+04 
    sync_Allgatherv                1    -       0.000     0.000     0.000       -       
    MPI_Allgatherv                 1    -       0.000     0.000     0.000     2.048e+04 
Total calls           = 30
Total recursive calls = 0
</div>
</pre>
<h3>Explanation of the above output</h3>
All of this output was generated automatically. Other than some optional
calls to <b>gptlsetoption()</b>, no GPTL-specific modifications to the
application code were made.
Timings for various MPI calls were automatically generated because <b>GPTL</b> was
built with <b>ENABLE_PMPI</b> (see macros.make.linux). A wrapping region
named MPI_Init_thru_Finalize was automatically generated for the region
between <b>MPI_Init()</b> and <b>MPI_Finalize()</b>. Average number of bytes
transferred was also reported automatically (see AVG_MPI_BYTES in the above
output). Also, since <b>gptlsync_mpi</b> was set (see code above), synchronization
routine <b>MPI_Barrier()</b> was called prior to certain collectives, and
synchronization time reported.
<hr />
<a href="example5.html"><img border="0" src="btn_previous.gif"
				  width="100" height="20" alt="Example 3"
				  /></a> 
<a href="gptl_homepage.html"><img border="0" src="btn_next.gif"
			     width="100" height="20" alt="GPTL home page" /></a>

<br />

</html>
