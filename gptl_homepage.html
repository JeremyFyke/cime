<html>
<head>
<title>GPTL Home Page</title>
<meta name="Author" content="Jim Rosinski">
<meta name="Keywords" content="gptl","timing","papi", "performance analysis">
<h1>GPTL - General Purpose Timing Library</h1>
<h2>(with optional PAPI interface)</h2>
</head>
<body bgcolor="peachpuff">
<body bgcolor="lightblue">

<b>GPTL</b> is a library to instrument C, C++, and Fortran codes for
    performance analysis, profiling, and generating a dynamic call tree. You can download
    it <A HREF="latest.tgz">here</A>. 
    The instrumentation can be done
    manually by inserting calls to <b>GPTLstart()</b> and
    <b>GPTLstop()</b> around regions of interest, or it can be done automatically at 
    function entry and exit 
    points if the application to be profiled is built with either 
    the GNU or PathScale compilers. Auto-instrumentation is accomplished by adding the
    option <b><em>-finstrument-functions</em></b> to the application's compile
    flags, then linking
    with <b><em>-lgptl</em></b>. See <A HREF="#EXAMPLES">EXAMPLES</A> below
    for some sample codes which use the library.
<p>
    If the <a HREF="http://icl.cs.utk.edu/papi">PAPI</a> library is
    available on the target platform, <b>GPTL</b> can be used to
    access all available <b>PAPI</b> events (including native events).
    To count floating point operations for example, one need only add
    a call that looks like: 

    <pre>
    ret = GPTLsetoption (PAPI_FP_OPS, 1);
    </pre>

    The second argument "1" in the above call means "enable". Any non-zero
    integer means "enable", and a zero means "disable".
    Multiple <b>GPTL</b> or <b>PAPI</b> options can be specified with additional
    calls to <b>GPTLsetoption()</b>. The man pages provided with the
    distribution describe the full API specification for each <b>GPTL</b>
    function. Except for the case-insensitivity of Fortran, the
    interface is identical for both Fortran and C/C++ 
    codes. 
<p>
    Calls to <b>GPTLstart()</b> and <b>GPTLstop()</b> can be nested to an
    arbitrary depth. <b>GPTL</b> handles multiply nested regions by
    presenting output in an indented fashion. If auto-instrumentation
    is used, this provides an easy way to get a dynamic call tree of
    the application being profiled.

<h2>Features</h2>
<ul>
<li> Low overhead.
<li> No external dependencies (<b>PAPI</b> interface is optional).
<li> Automatically multiplexes requested <b>PAPI</b> counters when required.
<li> Thread-safe, and reports per-thread statistics for multi-threaded
	codes.
<li> Includes utility functions to print memory usage
	(<b>GPTLprint_memusage()</b>) and get timestamps (<b>GPTLstamp()</b>).
<li> Includes utility scripts to post-process multi-threaded and
	multi-tasked output for easy assessment of load balance
	characteristics.
<li> Support for derived (<b>PAPI</b>-based) events such as computational
	intensity and instructions per cycle.
</ul>

<h2>Download and Installation</h2>
<ul> 
<li> The most recent release is <a href="gptl3_2.tar.gz">gptl3_2.tar.gz</a>
<li> To build and install <b>GPTL</b>, see the <A
	      HREF="INSTALL">INSTALL</a> file. Unfortunately I
	    don't know GNU Autoconf well enough to write a good "configure"
	    procedure, so you'll need to create a "macros.make" file 
	    appropriate for your target platform. Example macros.make files for various
	    architectures are included in the tar file. An autoconf-based
	    script named "suggestions" is included to help in editing
	    this file.
<li> For information on using <b>GPTL</b>, refer
	      to <A HREF="EXAMPLES">EXAMPLES</A> below, and the
	      man pages provided with the distribution.
</ul>

<HR SIZE=2 WIDTH="100%" ALIGN="LEFT" NOSHADE>
<A name=EXAMPLES></A>
<h2>Examples</h2>
Below are three simple codes which illustrate the use of some features of
	<b>GPTL</b>. They were all run on a Linux x86 using GNU
	compilers. <A
	  HREF="#EXAMPLE1">Example 1</A> is a manually-instrumented
	Fortran code which uses <b>PAPI</b> to count floating point
	operations. <A HREF="#EXAMPLE2">Example 2</A> is C code compiled
	with gcc's auto-instrumentation hooks to print a dynamic call tree. <A
	  HREF="#EXAMPLE3">Example 3</A> is a simple MPI code, the
	output of which is post-processed using Perl script
	<em><b>parsegpltout.pl</b></em> to examine load imbalance.
<HR SIZE=2 WIDTH="100%" ALIGN="LEFT" NOSHADE>

<A name=EXAMPLE1></A>
<h3>Example 1:</h3>
This is an OpenMP Fortran
	code manually instrumented with <b>GPTL</b> calls. The 
	output produced by the embedded call to <b>gptlpr()</b> is
	then shown and explained.
<p>
<b><em>papiomptest.f90:</em></b>
<pre>
<div style="background-color:white;">
program papiomptest
  implicit none
  include 'gptl.inc'                 ! Fortran GPTL include file
  include 'f90papi.h'                ! Needed for PAPI_FP_OPS
  integer :: ret, iter
  integer, parameter :: nompiter = 2 ! Number of OMP threads

  ret = gptlsetoption (gptlabort_on_error, 1) ! Abort on GPTL error
  ret = gptlsetoption (PAPI_FP_OPS, 1)        ! Count floating point ops
  ret = gptlsetoption (gptlnarrowprint, 1)    ! Print fewer sig figs
  ret = gptlsetoption (gptloverhead, 0)       ! Turn off overhead estimate
  ret = gptlinitialize ()                     ! Initialize GPTL
  ret = gptlstart ('total')                   ! Start a timer

!$OMP PARALLEL DO PRIVATE (iter)   ! Threaded loop
  do iter=1,nompiter
    ret = gptlstart ('A')          ! Start a timer
    ret = gptlstart ('B')          ! Start another timer
    ret = gptlstart ('C')
    call sleep (iter)              ! Sleep for "iter" seconds
    ret = gptlstop ('C')           ! Stop a timer
    ret = gptlstart ('CC')
    ret = gptlstop ('CC')
    ret = gptlstop ('A')
    ret = gptlstop ('B')         
  end do
  ret = gptlstop ('total')
  ret = gptlpr (0)                 ! Print timer stats
  ret = gptlfinalize ()            ! Clean up
end program papiomptest
</div>
</pre>

Compile and link, then run:
<pre>
% gfortran -fopenmp papiomptest.f90 -I/usr/local/include -lgptl -lpapi 
% env OMP_NUM_THREADS=2 ./a.out
</pre>

The call to gptlpr wrote a file named timing.0, which looks like this:

<pre>
<div style="background-color:white;">
PAPI event multiplexing was OFF
PAPI events enabled (including derived):
  Floating point operations executed

Underlying timing routine was gettimeofday.
Per-call utr overhead est: 2.9e-07 sec.
Per-call PAPI overhead est: 1.4e-07 sec.
If overhead stats are printed, roughly half the estimated number is
embedded in the wallclock (and/or PAPI counter) stats for each timer

If a '% of' field is present, it is w.r.t. the first timer for thread 0.
If a 'e6 per sec' field is present, it is in millions of PAPI counts per sec.

A '*' in column 1 below means the timer had multiple parents, though the
values printed are for all calls. Further down the listing is more detailed
information about multiple parents. Look for 'Multiple parent info'

Stats for thread 0:
             Called Recurse Wallclock max       min       % of total   FP_OPS e6 / sec 
  total             1   -       2.000     2.000     2.000     100.00       59     0.00 
    A               1   -       1.000     1.000     1.000      50.00       32     0.00 
      B             1   -       1.000     1.000     1.000      50.00       36     0.00 
        C           1   -       1.000     1.000     1.000      50.00        4     0.00 
        CC          1   -       0.000     0.000     0.000       0.00        4     4.00 
Total calls           = 5
Total recursive calls = 0

Stats for thread 1:
        Called Recurse Wallclock max       min       % of total   FP_OPS e6 / sec 
  A            1   -       2.000     2.000     2.000     100.00       50     0.00 
    B          1   -       2.000     2.000     2.000     100.00       54     0.00 
      C        1   -       2.000     2.000     2.000     100.00       22     0.00 
      CC       1   -       0.000     0.000     0.000       0.00        4     4.00 
Total calls           = 4
Total recursive calls = 0

Same stats sorted by timer for threaded regions:
Thd      Called Recurse Wallclock max       min       % of total   FP_OPS e6 / sec 
000 A           1   -       1.000     1.000     1.000      50.00       32     0.00 
001 A           1   -       2.000     2.000     2.000     100.00       50     0.00 
SUM A           2   -       3.000     2.000     1.000     150.00       82     0.00 

000 B           1   -       1.000     1.000     1.000      50.00       36     0.00 
001 B           1   -       2.000     2.000     2.000     100.00       54     0.00 
SUM B           2   -       3.000     2.000     1.000     150.00       90     0.00 

000 C           1   -       1.000     1.000     1.000      50.00        4     0.00 
001 C           1   -       2.000     2.000     2.000     100.00       22     0.00 
SUM C           2   -       3.000     2.000     1.000     150.00       26     0.00 

000 CC          1   -       0.000     0.000     0.000       0.00        4     4.00 
001 CC          1   -       0.000     0.000     0.000       0.00        4     4.00 
SUM CC          2   -       0.000     0.000     0.000       0.00        8     4.00 
</div>
</pre>

<h3>Explanation of the above output</h3>
The output file contains a preamble which lists <b>PAPI</b>
settings such as whether multiplexing was on or off, and
which <b>PAPI</b> events were enabled. In this case 
"Floating point operations executed" were counted. Other preamble contents
include estimates of underlying timing routine (UTR)
overhead, <b>PAPI</b> overhead, and explanations of the printed
statistics. 
<p>
The statistics themselves begin with the line which reads "Stats for
thread 0:". The region names are listed on the far left. A 
"region" is defined in the application by calling
<b>GPTLstart()</b>, then <b>GPTLstop()</b> for the same input (character
string) argument.
Indenting of 
the names preserves parent-child relationships between the regions. In
the example, we see that region "A" was contained in "total", "B"
contained in "A", and regions "C" and "CC" both contained in "B". 
<p>
Reading across the output from left to right, the next column is labelled
"Called". This is the number of times the region was invoked. If any regions
were called recursively, that information is printed next. In this case there
were no recursive calls so just a "-" is printed. Total wallclock time for
each region is printed next, followed by the max and min values for any
single invocation. In this simple example each region was called only once, so
"Wallclock", "max", and "min" are all the same. The next column lists the
percentage of wallclock time each region took compared to the very first
region timed. This is generally useful only if there is a single region
wrapping the entire execution ("total" in the above example). This output can
be turned off with a call that looks like <b>GPTLsetoption (GPTLpercent, 0)</b>. 
PAPI-based statistics are presented next. In
the example we counted PAPI_FP_OPS. The name is shortened to FP_OPS to
confine the printed output to as few columns as possible. Finally, each PAPI
count is divided by wallclock time and printed as millions per second (in
this case millions of floating point operations per second). This column can
also be turned off, with a call to <b>GPTLsetoption (GPTLpersec, 0)</b>. 

<p>
Since this was a threaded code run with OMP_NUM_THREADS=2, statistics
for the second thread (see "Stats for thread 1:") are also printed. The
output shows that thread 1
participated in the computations for regions "A", "B", "C", and "CC", but not
"total". This is reflected in the code itself, since only the master
thread was active when region "total" was started and stopped.

<p>
After the per-thread statistics section, the same information is repeated, sorted by
region name if more than one thread was active. This section is delimited by
the string "Same stats sorted by
timer for threaded regions:". This makes it easier to inspect for load
balance across threads. The leftmost column is thread number, and the region
names are not indented.

<HR SIZE=2 WIDTH="100%" ALIGN="LEFT" NOSHADE>

<A name=EXAMPLE2></A>
<h3>Example 2:</h3>
The next example is a C code compiled with auto-instrumentation enabled. It
uses <b>PAPI</b> to count total instructions (a <b>PAPI</b>
preset event), and instructions per cycle (a derived event 
requiring <b>PAPI</b> counters). Note
that function <em>B</em> has multiple parents, and <b>GPTL</b> reports the
	multiple parent information in the output produced by the call
	to <b>GPTLpr_file()</b>. 
<p>
<b><em>main.c:</em></b>
<pre>
<div style="background-color:white;">
#include &#60gptl.h&gt
#include &#60papi.h&gt

int main ()
{
  void do_work (void);
  int i, ret;
  ret = GPTLsetoption (GPTL_IPC, 1);     // Count instructions per cycle
  ret = GPTLsetoption (PAPI_TOT_INS, 1); // Print total instructions
  ret = GPTLsetoption (GPTLoverhead, 0); // Don't print overhead estimate
  ret = GPTLinitialize ();               // Initialize GPTL
  ret = GPTLstart ("main");              // Start a manual timer
  do_work ();                            // Do some work 
  ret = GPTLstop ("main");               // Stop the manual timer
  ret = GPTLpr_file ("outfile");         // Write output to "outfile"
}
</div>
</pre>

<b><em>subs.c:</em></b>
<div style="background-color:white;">
<pre>
#include &#60unistd.h&gt

extern void A(void);
extern void AA(void);
extern void B(void);

void do_work ()
{
  A ();
  AA ();
  B ();
}

void A ()
{
  B ();
}

void AA ()
{
}

void B ()
{
  sleep (1);
}
</div>
</pre>
Compile all but <em>main.c</em> with auto-instrumentation, then link and
run. Useful auto-instrumentation of the main program is not possible,
because the call to <b>GPTLinitialize()</b> must be done manually and
needs to preceed all calls to <b>GPTLstart</b> and <b>GPTLstop</b>. 
<pre>
% gcc -c main.c
% gcc -finstrument-functions subs.c main.o -lgptl -lpapi
% ./a.out
</pre>

Now convert the auto-instrumented output to human-readable form:
<pre>
% hex2name.pl a.out outfile > outfile.converted
</pre>

Output file <em> outfile.converted</em> looks like this:
<pre>
<div style="background-color:white;">
PAPI event multiplexing was OFF
PAPI events enabled (including derived):
  Instructions per cycle
  Total instructions executed

Underlying timing routine was gettimeofday.
Per-call utr overhead est: 2.8e-07 sec.
Per-call PAPI overhead est: 1.4e-07 sec.
If overhead stats are printed, roughly half the estimated number is
embedded in the wallclock (and/or PAPI counter) stats for each timer

If a '% of' field is present, it is w.r.t. the first timer for thread 0.
If a 'e6 per sec' field is present, it is in millions of PAPI counts per sec.

A '*' in column 1 below means the timer had multiple parents, though the
values printed are for all calls. Further down the listing is more detailed
information about multiple parents. Look for 'Multiple parent info'

Stats for thread 0:
                     Called Recurse Wallclock max       min       % of  main GPTL_IPC  TOT_INS e6 / sec 
  main                     1    -       2.000     2.000     2.000     100.00 1.31e-01    16654     0.01 
    do_work                1    -       2.000     2.000     2.000     100.00 1.20e-01    12278     0.01 
      A                    1    -       1.000     1.000     1.000      50.00 1.31e-01     4910     0.00 
*       B                  2    -       2.000     1.000     1.000     100.00 5.14e-02     2739     0.00 
      AA                   1    -       0.000     0.000     0.000       0.00 3.14e-01      429   429.00 
Total calls           = 6
Total recursive calls = 0

Multiple parent info (if any) for thread 0:
The 2 columns are number of child invocations
The rows are each parent, with their common child being the last entry, which is indented
Counts next to parents are number of times they called the child
Count next to child is total number of times it was called
       1 A                         
       1 do_work                         
       2   B                         
</div>
</pre>
<h3>Explanation of the above output</h3>
Compared to the output from <A HREF="#EXAMPLE1">Example 1</A> above, there
are some differences. In addition to "Total instructions executed"
(PAPI_TOT_INS), derived event GPTL_IPC ("Instructions per
cycle") was also enabled. To compute this metric, PAPI counter PAPI_TOT_CYC
also needed to be enabled. This was done automatically by <b>GPTL</b>.
<p>
Note the asterisk in front of region "B". This
indicates that region "B" had multiple parents. It is presented as a child of
region "A" because that is the first region that invokes it. Information
about other parents is presented after the main call tree. It shows that
region "B" had two parents, "A", and "do_work". Each parent invoked "B" once,
for a total of 2 calls.

<HR SIZE=2 WIDTH="100%" ALIGN="LEFT" NOSHADE>

<A name=EXAMPLE3></A>
<h3>Example 3:</h3>
This hybrid OpenMP/MPI code simulates load imbalance at both the
	thread and task level by sleeping a number of seconds equal to
	the thread number plus the MPI rank, within a region being
	timed. Utility script 
	<em><b>parsegptlout.pl</b></em> is used after the code has
	been run to examine timing statistics
	for the region.
<p>
<b><em>mpi.c:</em></b>
<pre>
<div style="background-color:white;">
#include &#60unistd.h&gt
#include &#60omp.h&gt
#include &#60mpi.h&gt
#include &#60gptl.h&gt

int main (int argc, char **argv)
{
  int i, ret, iam;
  int nompiter = omp_get_max_threads ();       // Loop trip count matches number of threads

  ret = MPI_Init (&argc, &argv);               // Initialize MPI
  ret = MPI_Comm_rank (MPI_COMM_WORLD, &iam);  // Get my rank

  ret = GPTLsetoption (GPTLoverhead, 0);       // Don't print overhead stats
  ret = GPTLsetoption (GPTLabort_on_error, 1); // Abort on any GPTL error

  ret = GPTLinitialize ();                     // Initialize GPTL
  ret = GPTLstart ("total");                   // Time the whole program

  /* Threaded loop with load imbalance across both OMP threads and MPI tasks */

#pragma omp parallel for private (i, ret)
  for (i = 0; i < nompiter; i++) {
    ret = GPTLstart ("sleep_iam_plus_mythread");// Start timer for simulated work
    ret = sleep (iam+i);                        // Load-imbalanced work
    ret = GPTLstop ("sleep_iam_plus_mythread"); // Stop timer for simulated work
  }

  ret = GPTLstart ("barriersync");             // Time MPI task synchronization
  ret = MPI_Barrier (MPI_COMM_WORLD);          // Synchronize MPI tasks
  ret = GPTLstop ("barriersync");

  ret = GPTLstart ("sleep_1");                // This region has a balanced load
  ret = sleep (1);
  ret = GPTLstop ("sleep_1");
  ret = GPTLstop ("total");

  ret = GPTLpr (iam);                          // Print the results
  ret = MPI_Finalize ();                       // Clean up MPI
  return 0;
}
</div>
</pre>

Compile and link, then run with 2 threads and 3 MPI tasks:
<pre>
% gcc -fopenmp mpi.c -lgptl 
% env OMP_NUM_THREADS=2 mpiexec -n 3 ./a.out
</pre>

Output files <em>timing.0</em>, <em>timing.1</em>, and <em>timing.2</em> are created by the call
to <b>GPTLpr()</b>, one for each MPI task. First let's examine the <b>GPTL</b> output
file for task 0. Preamble and postamble print has been deleted for
brevity.
<p>
<b><em>timing.0:</em></b>
<pre>
<div style="background-color:white;">
Stats for thread 0:
                           Called Recurse Wallclock max       min       % of total 
  total                           1   -       4.003     4.003     4.003     100.00 
    sleep_iam_plus_mythread       1   -       0.000     0.000     0.000       0.00 
    barriersync                   1   -       2.001     2.001     2.001      49.99 
    sleep_1                       1   -       1.000     1.000     1.000      24.99 
Total calls           = 4
Total recursive calls = 0

Stats for thread 1:
                         Called Recurse Wallclock max       min       % of total 
  sleep_iam_plus_mythread       1   -       1.000     1.000     1.000      24.98 
Total calls           = 1
Total recursive calls = 0

Same stats sorted by timer for threaded regions:
Thd                        Called Recurse Wallclock max       min       % of total 
000 sleep_iam_plus_mythread       1   -       0.000     0.000     0.000       0.00 
001 sleep_iam_plus_mythread       1   -       1.000     1.000     1.000      24.98 
SUM sleep_iam_plus_mythread       2   -       1.000     1.000     0.000      24.98 
</div>
</pre>

The results are fairly straightforward and as expected. Note
though that for thread 0, region "total" took 4 seconds, while
the sum of all the other regions it surrounded adds up to only
3. What's going on? The reason is that
there is an implied (and untimed) OpenMP barrier before the end of the threaded loop.
Thread 0 slept 0 seconds, then had to wait for thread 1 to finish
sleeping for 1 second before it exited the loop.
<p>
Next we'll use <em><b>parsegpltout.pl</b></em> to examine the
timing files for all MPI tasks and gather load balance statistics for one of the regions
being timed:
<pre>
% parsegptlout.pl sleep_iam_plus_mythread
</pre>
Here's the output:
<pre>
<div style="background-color:white;">
Found 6 values spread across 3 tasks
Heading is Wallclock
Max   =  3.000 on thread 1 task 2
Min   =  0.000 on thread 0 task 0
Mean  = 1.50016666666667
Total = 9.001
</div>
</pre>
We see that the max time for this region was spent in thread 1, task
2. The three seconds reported is due to the call
to <em>sleep(iam+mythread)</em>. Likewise, thread 0 on task 0 spent
the least amount of time in this region, 0 seconds.

<HR SIZE=2 WIDTH="100%" ALIGN="LEFT" NOSHADE>

<h2>Bug Reports</h2>
Please <a HREF="mailto:rosinskijm@ornl.gov">email</a> me bug reports
	and/or feature requests.

<h2>Author</h2>
<b>GPTL</b> was written
by <a HREF="http://www.burningserver.net/rosinski">Jim Rosinski</a>,
currently at <a HREF="http://www.ornl.gov">ORNL</a>, formerly
  of <a HREF="http://www.sicortex.com">SiCortex</a>,
and <a HREF="http://www.ncar.edu">NCAR</a>. 
<h2>Copyright</h2>
This software is <b>Open Source</b>. My only request is that you don't
embed it in software that you intend to sell.
</html>
